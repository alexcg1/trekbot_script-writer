{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Star Trek Script Generator\n\nBased on [script_generation.ipynb](https://github.com/cdpierse/script_buddy_v2/blob/master/script_buddy/script_generation.ipynb)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup, AutoTokenizer, AutoModelWithLMHead\nimport numpy as np\nimport os\nimport random\nfrom datetime import datetime\nfrom IPython.display import clear_output\nfrom time import sleep\nfrom zipfile import ZipFile\nimport subprocess\nfrom utils import *","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.chdir(\"/kaggle/working\") # Make sure we're in base directory","execution_count":2,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## User Settings"},{"metadata":{"trusted":true},"cell_type":"code","source":"# How many episodes to train on? If 0, assume all episodes\nEP_COUNT = 0\n\n# How many times to cycle through all episodes in training?\nEPOCHS = 1\n\n# How many batches to run together?\nBATCH_SIZE = 1\n\n# Testing string. Trekbot starts with this to create a new script\nSAMPLE_STRING = \"Picard to Riker\"\n\n# Sample length for samples generated from model\nSAMPLE_LENGTH = 500\n\n# What text should we train on? Default is Star Trek episodes\n# TRAINING_FILE_PATH = os.path.join(\"..\", \"input\", \"trekbot\", \"film_text.txt\")","execution_count":3,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Check Input File\n\nUncomment the below lines to see the first few lines of your input file"},{"metadata":{"trusted":true},"cell_type":"code","source":"# with open(TRAINING_FILE_PATH, \"r\") as file:\n#     print(file.read()[0:400])","execution_count":4,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### End User Settings"},{"metadata":{"trusted":true},"cell_type":"code","source":"MODEL_NAME = 'trekbot'\nMODEL_DIR = os.path.join(\"..\", \"input\", \"trekbot-model\")\n\nFILENAME_SUFFIX = str(datetime.now())[:10]\n\n# Filename for output scripts\nSCRIPT_DIR = f\"scripts/\"\nSCRIPT_FILENAME = f\"scripts-{FILENAME_SUFFIX}.txt\"\n\n# Directory to save model\nOUTPUT_MODEL_DIR = f\"models/trekbot\"\nOUTPUT_MODEL_FILENAME = f\"model.txt\"\n\n# This variable already used in code later\noutput_dir = OUTPUT_MODEL_DIR","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dirs = ['models', 'scripts', SCRIPT_DIR, 'samples', 'diagnostics']\n\nSetup.dir_setup(dirs) # Setup directory structure","execution_count":6,"outputs":[{"output_type":"stream","text":"Created models\nCreated scripts\nSkipped scripts/. It already exists\nCreated samples\nCreated diagnostics\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = 'cpu'\nif torch.cuda.is_available():\n    device = 'cuda'","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Using model {MODEL_NAME} from {MODEL_DIR}\")\nmodel = GPT2LMHeadModel.from_pretrained(MODEL_DIR)","execution_count":8,"outputs":[{"output_type":"stream","text":"Using model trekbot from ../input/trekbot-model\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Load Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = model.to(device)","execution_count":9,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load Dataset\n\nIt's already pickled for fast loading"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\npickle_path = os.path.join(\"..\", \"input\", \"trekbot\", \"gpt2_trekbot.txt\")\ndataset = pickle.load(open(pickle_path, \"rb\"))\nprint(f\"Dataset has {len(dataset)} scripts total\")","execution_count":10,"outputs":[{"output_type":"stream","text":"Dataset has 13051 scripts total\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Set episode count\n\nFor testing the whole pipeline works, I'm just running it with a few episodes to start with"},{"metadata":{"trusted":true},"cell_type":"code","source":"if EP_COUNT != 0:\n    dataset = dataset[:EP_COUNT]","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"script_loader = DataLoader(dataset,batch_size=1,shuffle=True)\nif EP_COUNT != 0:\n    print(f\"Loaded {EP_COUNT} scripts from dataset\")\nelse:\n    print(\"Loaded all scripts from dataset\")","execution_count":12,"outputs":[{"output_type":"stream","text":"Loaded all scripts from dataset\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"LEARNING_RATE = 0.00006 # Faster uses more GPU?\nWARMUP_STEPS = 10000","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = model.to(device)\nmodel.train()\noptimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=WARMUP_STEPS, num_training_steps=-1)\nscript_count = 0\nsum_loss = 0.0\nbatch_count = 0","execution_count":14,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"total = len(dataset) # number of items in dataset","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from math import floor\n\nSAMPLE_INTERVAL = 2000\ntotal_sample_count = floor(total/SAMPLE_INTERVAL)\nprint(f\"Total samples: {total_sample_count}\")","execution_count":16,"outputs":[{"output_type":"stream","text":"Total samples: 6\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"start_time = datetime.now().strftime(\"%H:%M:%S\")\nprint(f\"Start: {start_time}\")","execution_count":17,"outputs":[{"output_type":"stream","text":"Start: 00:39:49\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Setup diagnostics\nlosses = []\nloss_log = \"loss_rate.csv\"\n\nDiag.setup(loss_log)","execution_count":18,"outputs":[{"output_type":"stream","text":"Creating new log at /kaggle/working/diagnostics/loss_rate.csv\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = GPT2Tokenizer.from_pretrained(MODEL_DIR)","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.chdir(SCRIPT_DIR)","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%timeit\nfor epoch in range(EPOCHS):\n    print(f\"EPOCH {epoch} started\" + '=' * 30)\n    for idx,script in enumerate(script_loader):\n                \n        outputs = model(script.to(device), labels=script.to(device))\n        \n        loss, logits = outputs[:2]\n        timestamp = datetime.now().strftime(\"%H:%M:%S\")\n        \n        loss.backward()\n\n        sum_loss = sum_loss + loss.detach().data\n                       \n        script_count = script_count + 1\n        if script_count == BATCH_SIZE:\n            script_count = 0    \n            batch_count += 1\n            optimizer.step()\n            scheduler.step() \n            optimizer.zero_grad()\n            model.zero_grad()\n        \n        if batch_count == SAMPLE_INTERVAL:\n            append_string = f\"\\t| Appending script to {SCRIPT_FILENAME}...\"\n        else:\n            append_string = ''\n        \n        # Update output display\n        clear_output(wait=True) # Clear and update display, otherwise endless scroll\n        percent = round(idx/total, 5)*100\n        rounded_loss = round(float(loss.detach().data), 4)\n        rounded_sum_loss = round(float(sum_loss), 2)\n        print(f\"{timestamp}: Processing {idx}/{total} \\t {percent}% \\tLoss: {rounded_loss} | Sum loss: {rounded_sum_loss} {append_string}\")\n        if batch_count == SAMPLE_INTERVAL:\n            print(\"Saving script\")\n            model.eval()\n            losses.append(sum_loss)\n            sample_outputs = model.generate(\n                                    bos_token_id=random.randint(1,30000),\n                                    do_sample=True,   \n                                    top_k=50, \n                                    max_length = SAMPLE_LENGTH,\n                                    top_p=0.95, \n                                    num_return_sequences=1\n                                )\n\n            for i, sample_output in enumerate(sample_outputs):\n                with open(SCRIPT_FILENAME, \"a\") as file:\n                    file.write(\"\\n\\n\")\n                    file.write(\"*\" * 80)\n                    file.write(\"\\n\\n\")\n                    file.write(tokenizer.decode(sample_output, skip_special_tokens=True))\n                    print(f\"Script appended to {SCRIPT_FILENAME}\")\n            \n            batch_count = 0\n            sum_loss = 0.0\n            model.train()","execution_count":21,"outputs":[{"output_type":"stream","text":"EPOCH 0 started==============================\n","name":"stdout"},{"output_type":"error","ename":"UnboundLocalError","evalue":"local variable 'sum_loss' referenced before assignment","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-45449e9288e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'timeit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'for epoch in range(EPOCHS):\\n    print(f\"EPOCH {epoch} started\" + \\'=\\' * 30)\\n    for idx,script in enumerate(script_loader):\\n                \\n        outputs = model(script.to(device), labels=script.to(device))\\n        \\n        loss, logits = outputs[:2]\\n        timestamp = datetime.now().strftime(\"%H:%M:%S\")\\n        \\n        loss.backward()\\n\\n        sum_loss = sum_loss + loss.detach().data\\n                       \\n        script_count = script_count + 1\\n        if script_count == BATCH_SIZE:\\n            script_count = 0    \\n            batch_count += 1\\n            optimizer.step()\\n            scheduler.step() \\n            optimizer.zero_grad()\\n            model.zero_grad()\\n        \\n        if batch_count == SAMPLE_INTERVAL:\\n            append_string = f\"\\\\t| Appending script to {SCRIPT_FILENAME}...\"\\n        else:\\n            append_string = \\'\\'\\n        \\n        # Update output display\\n        clear_output(wait=True) # Clear and update display, otherwise endless scroll\\n        percent = round(idx/total, 5)*100\\n        rounded_loss = round(float(loss.detach().data), 4)\\n        rounded_sum_loss = round(float(sum_loss), 2)\\n        print(f\"{timestamp}: Processing {idx}/{total} \\\\t {percent}% \\\\tLoss: {rounded_loss} | Sum loss: {rounded_sum_loss} {append_string}\")\\n        if batch_count == SAMPLE_INTERVAL:\\n            print(\"Saving script\")\\n            model.eval()\\n            losses.append(sum_loss)\\n            sample_outputs = model.generate(\\n                                    bos_token_id=random.randint(1,30000),\\n                                    do_sample=True,   \\n                                    top_k=50, \\n                                    max_length = SAMPLE_LENGTH,\\n                                    top_p=0.95, \\n                                    num_return_sequences=1\\n                                )\\n\\n            for i, sample_output in enumerate(sample_outputs):\\n                with open(SCRIPT_FILENAME, \"a\") as file:\\n                    file.write(\"\\\\n\\\\n\")\\n                    file.write(\"*\" * 80)\\n                    file.write(\"\\\\n\\\\n\")\\n                    file.write(tokenizer.decode(sample_output, skip_special_tokens=True))\\n                    print(f\"Script appended to {SCRIPT_FILENAME}\")\\n            \\n            batch_count = 0\\n            sum_loss = 0.0\\n            model.train()\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2360\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2361\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2362\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2363\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1158\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m                 \u001b[0mnumber\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1160\u001b[0;31m                 \u001b[0mtime_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1161\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtime_number\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, number)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0mtiming\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mgcold\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<magic-timeit>\u001b[0m in \u001b[0;36minner\u001b[0;34m(_it, _timer)\u001b[0m\n","\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'sum_loss' referenced before assignment"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"end_time = datetime.now().strftime(\"%H:%M:%S\")\nprint(f\"Start: {start_time}\")\nprint(f\"End: {end_time}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Save Trained Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"os.chdir(\"/kaggle/working\")\nos.mkdir(OUTPUT_MODEL_DIR)\n\nfrom transformers import WEIGHTS_NAME, CONFIG_NAME\noutput_model_file = os.path.join(OUTPUT_MODEL_DIR, WEIGHTS_NAME)\noutput_config_file = os.path.join(OUTPUT_MODEL_DIR, CONFIG_NAME)\n\ntorch.save(model.state_dict(), output_model_file)\nprint(f\"Saved {output_model_file} to {OUTPUT_MODEL_DIR}\")\nmodel.config.to_json_file(output_config_file)\nprint(f\"Saved {output_config_file} to {OUTPUT_MODEL_DIR}\")\ntokenizer.save_vocabulary(OUTPUT_MODEL_DIR)\nprint(f\"Saved vocabulary to {OUTPUT_MODEL_DIR}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Test Trained Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = GPT2LMHeadModel.from_pretrained(output_dir)\ntokenizer = GPT2Tokenizer.from_pretrained(output_dir)\nprint(f\"Loaded model and tokenizer from {output_dir}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_outputs = model.generate(\n    bos_token_id=random.randint(1,30000),\n    do_sample=True,   \n    top_k=50, \n    max_length = 1000,\n    top_p=0.95, \n    num_return_sequences=5\n    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generate samples\nfor i, output in enumerate(sample_outputs):\n    filename = f'script_{i+1:03}.txt'\n    file_path = f'samples/{filename}'\n    content = tokenizer.decode(output, skip_special_tokens=True)\n    with open(file_path, 'w') as file:\n        file.write(content)\n    print(f\"{filename} written\")\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Zip samples\nos.chdir('samples')\nwith ZipFile('samples.zip', 'w') as zipObj:\n    for filename in os.listdir():\n        if not filename.endswith(\".zip\"):\n            zipObj.write(filename)\n\nprint(\"samples.zip created\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Start loss {losses[0]}\")\nprint(f\"End loss {losses[-1]}\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}